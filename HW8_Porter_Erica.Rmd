---
title: "HW8_Porter_Erica"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---
```{r setup, include = F, message = F, warning = F}
library(tidyr, quietly = T, warn.conflicts = F)
library(dplyr, quietly = T, warn.conflicts = F)
library(stringr, quietly = T,warn.conflicts = F)
library(tidytext, quietly = T, warn.conflicts = F)
library(tidyselect, quietly = T, warn.conflicts = F)
library(tidyverse, quietly = T, warn.conflicts = F)
library(tm, quietly = T, warn.conflicts = F)
library(ggplot2, quietly = T, warn.conflicts = F)
library(RColorBrewer, quietly = T, warn.conflicts = F)
library(wordcloud, quietly = T, warn.conflicts = F)
library(data.table, quietly = T, warn.conflicts = F)
library(lettercase, quietly = T, warn.conflicts = F)
library(jsonlite, quietly = T, warn.conflicts = F)
library(ggraph, quietly = T, warn.conflicts = F)
library(igraph, quietly = T, warn.conflicts = F)
library(widyr, quietly = T, warn.conflicts = F)
library(GGally, quietly = T, warn.conflicts = F)
library(network, quietly = T, warn.conflicts = F)
library(sna, quietly = T, warn.conflicts = F)
library(geomnet, quietly = T, warn.conflicts = F)
```


#Problem 1
I have a new repository with the name EMP_Stats under my username, ericamp.  The link to my new repository is https://github.com/ericamp/EMP_Stats


#Problem 2
Load munge and create a story for the class dataset: survey_data.txt.  The file survey_data.txt contains columns of information originally written by hand by each of us in the course on the first day of class.  Therefore, the responses vary significantly in format, content, and length.  

* Goal: use tidy concepts

    + Each variable is a column
    + Each observation is a row
    + Each type of observational unit is a table
    
Here I decided to inspect and analyze/plot aspects of the last column in the data, "Other Programming," since it seems to contain the largest amount of unique words and information.  However, the following process (and more) could be performed on the entire data set.  I first read the survey_data.txt file into a data frame using $\texttt{read.table}$ and removed uppercase letters with $\texttt{tolower}$ to make analysis easier.  Next, I used several $\texttt{gsub}$ statements to remove punctuation and instances of two or more spaces.  Finally, I used $\texttt{strsplit}$ to separate the words in each row and $\texttt{unlist}$ to turn the list into a vector.  It certainly wasn't the most elegant way to go about things, but it allowed me to then create a word cloud and barplot of the most commonly occuring words in the "Other Programming" column.  The word cloud would be more appealing for a larger data set with more words to include, but this one certainly demonstrates the general visual and possible execution.  I have included the portions of code below that were used to read in the initial data and create the graphics.  I've also included a short table of the 6 words/programming languages that appear most frequently in the data set.


```{r text1, echo = T, eval = T}
# read in the data and remove all uppercase

text <- read.table("~/Desktop/texts/survey_data.txt", header = T, sep = "\t")
text[] <- lapply(text, tolower)

```


```{r text2, echo = F, eval = T}
# use gsub to remove punctuation and extra spaces

text[]$Other.programming <- gsub(",", " ", text[]$Other.programming)
text[]$Other.programming <- gsub("  ", " ", text[]$Other.programming)
text[]$Other.programming <- gsub("/", " ", text[]$Other.programming)
text[]$Other.programming <- gsub("-", " ", text[]$Other.programming)
text[]$Other.programming <- gsub("\\(", " ", text[]$Other.programming)
text[]$Other.programming <- gsub("\\)", " ", text[]$Other.programming)
text[]$Other.programming <- gsub("  ", " ", text[]$Other.programming)

# split strings into individual words to analyze

text[]$Other.programming <- strsplit(text[]$Other.programming, " ")

# table of frequent words

word_list <- unlist(text[]$Other.programming)
words <- as.data.frame((table(word_list)))
words <- words[order(-words$Freq),]
knitr::kable(head(words), caption = "Common Responses to Other Programming Experience")

```

```{r text3, echo = T, eval = T}

wordcloud(word = word_list, min.freq = 2, scale=c(7, 0.6), colors=brewer.pal(6, "Dark2"))

barplot(words[1:6,]$Freq, names.arg = words[1:6,]$word_list, horiz = T, las = 1, density = 45, angle = 11, col = "cornflowerblue", main = "Other Programming Experience", xlab = "Counts")

```


#Problem 3
Here I found another data set to analyze where the data is all text.  I initially found some other social media data sets, similar to the case study; these would have been super cool to analyze, but they were so large that I couldn't even open them without crashing my computer.  So, I found an unrelated data set which contains text from ads related to farm products; the data appears to have some irrelevant information mixed in, for example it includes words where there were originally photos in the ads (so there are irrelevant words like photo, video, header, etc.).  However, there aren't very many special characters or special cases/columns, so les cleaning is required.  Below I attempted to create a barplot, network graph, and a word cloud from the data.

Data: https://archive.ics.uci.edu/ml/machine-learning-databases/00218/

Citation: Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

```{r case_study, echo = F, eval = T}

# read in data

ads <- readLines("~/Desktop/farm-ads.txt")
ads_vec <- Corpus(VectorSource(ads))

# remove stop words and dashes

remove_punc <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
ads_vec <- tm_map(ads_vec, remove_punc, "-")
ads_vec <- tm_map(ads_vec, removeWords, stopwords("english"))
ads_vec <- tm_map(ads_vec, removeWords, c("title", "ad", "picture", "header", "view", "profile"))
ads_vec <- tm_map(ads_vec, stripWhitespace)

# find common words

dtm <- TermDocumentMatrix(ads_vec)
ads.mat <- as.matrix(dtm)
ads.mat <- sort(rowSums(ads.mat), decreasing=  TRUE)
ads.frame <- data.frame(word = names(ads.mat), freq = ads.mat)
knitr::kable(as.data.frame(head(ads.frame, 10)))

# correlations

health_cor <- as.data.frame(findAssocs(dtm, terms = "health", corlimit = 0.4))
health_cor <- as.data.frame(cbind(rownames(health_cor), health_cor$health))
colnames(health_cor) <- c("assoc_word", "Corr")
health_cor <- as.tibble(health_cor)

```

```{r graphs, echo = T, eval = F, include = T}
# My attempt at a network graph...

net = network(health_cor, directed = TRUE)
network.vertex.names(net) = as.vector(health_cor$assoc_word)
ggnet2(net, node.size = 6, node.color = "gray", edge.size = 1, edge.color = "grey", label = T)

# barplot of frequencies
barplot(ads.frame[1:10,]$freq, las = 2, names.arg = ads.frame[1:10,]$word,
        col = "lightpink", main = "Most frequent words in farm ads")

# wordcloud
wordcloud(words = as.vector(ads.frame$word), freq = as.vector(ads.frame$freq), min.freq = 2000, scale = c(4, 0.6), colors=brewer.pal(6, "Dark2"))

```


#Problem 4
I created an account at arc.vt.edu and I tested my login through SSH; the account is under my PID, $\textbf{ericamp}$.


\newpage

#Appendix

```{r Appendix, ref.label = c("text1", "text2", "text3", "case_study", "graphs"), eval = F}